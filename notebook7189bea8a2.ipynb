{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy\nimport cv2\nimport os\nfrom tqdm import tqdm","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/dynamic-sign-language-gestures-01/all\"\nimage_size = 256\nframe_rate = 0.1\n\ndef getFrame(sec):\n    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n    hasFrames,image = vidcap.read()\n    if hasFrames:\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.resize(image, (image_size, image_size))\n    return [hasFrames, image]","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"videos_list = sorted(os.listdir(path))[:500]\nnew_list = []\n\nfor xyz in tqdm(range(10)):\n    mno = xyz*50\n    for index in range(mno, mno+50):\n        new_list.append(videos_list[index])","execution_count":4,"outputs":[{"output_type":"stream","text":"100%|██████████| 10/10 [00:00<00:00, 45739.41it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = []\nlabels = []\n\nfor video in tqdm(new_list):\n#     vidcap = cv2.VideoCapture(path + '/' + video)\n#     sec = 0\n#     mini = []\n#     success, frame = getFrame(sec)\n#     while success:\n#         mini.append(frame)\n#         sec = sec + frame_rate\n#         sec = round(sec, 2)\n#         success, frame = getFrame(sec)\n        \n#     frames.append(mini[-8:])\n    labels.extend([int(video[:3]) -1]*8)","execution_count":14,"outputs":[{"output_type":"stream","text":"100%|██████████| 500/500 [00:00<00:00, 385718.59it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n# frames = numpy.array(frames)\nframes = numpy.load('../input/aslframes/last8-10classes.npy')\nlabels = numpy.array(labels)\ncat_labels = to_categorical(labels)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(frames, labels, test_size=0.1,\n                                                   shuffle=True, random_state=42, stratify=labels)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=30,\n    width_shift_range=0.2,\n#     height_shift_range=0.2,\n    zoom_range=[0.75,1.25])\ntrain_datagen.fit(x_train)\n\ntest_datagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=30,\n    width_shift_range=0.2,\n#     height_shift_range=0.2,\n    zoom_range=[0.75,1.25])\ntest_datagen.fit(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.layers import GlobalMaxPooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\n\nmodel = Xception(include_top=False)\nembed = GlobalMaxPooling2D()(model.output)\nlayer = Dropout(0.4)(embed)\nlayer = Dense(1024, activation='relu')(layer)\nlayer = Dropout(0.4)(layer)\nlayer = Dense(256, activation='relu')(layer)\nlayer = Dense(64, activation='relu')(layer)\noutput = Dense(10, activation='softmax')(layer)\n\nmodel = Model(inputs=model.inputs, outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ncheckpoint = tensorflow.keras.callbacks.ModelCheckpoint(filepath='model.h5', monitor='val_accuracy',\n                                                       save_best_only=True, verbose=1)\nearlyStop = tensorflow.keras.callbacks.EarlyStopping(patience=16, monitor='val_accuracy',\n                                                    restore_best_weights=True, verbose=1)\nreduceLR = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, \n                                                patience=4, verbose=1)\n\n# model.fit(train_datagen.flow(x_train, y_train, batch_size=32),\n#           steps_per_epoch=len(x_train) / 32, epochs=128, verbose=1,\n#           validation_data=(test_datagen.flow(x_test, y_test, batch_size=32)),\n#           callbacks=[checkpoint, earlyStop, reduceLR])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train, batch_size=32, epochs=128,\n          steps_per_epoch=len(x_train) / 32, verbose=1,\n          validation_data=(x_test, y_test),\n          callbacks=[checkpoint, earlyStop, reduceLR])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nacc = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\ncm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vidcap = cv2.VideoCapture('../input/dynamic-sign-language-gestures-01/all/001_001_004.mp4')\nsec = 0\nmini = []\nsuccess, frame = getFrame(sec)\nwhile success:\n    mini.append(frame)\n    sec = sec + frame_rate\n    sec = round(sec, 2)\n    success, frame = getFrame(sec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mini = numpy.array(mini[-8:])\ny_pred = model.predict(mini)\ny_pred = y_pred.argmax(axis=1)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2]*x_test.shape[3])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodel = XGBClassifier()\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nacc = accuracy_score(y_test, y_pred)\nacc","execution_count":22,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"[15:23:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","name":"stdout"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"1.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\nkappa = cohen_kappa_score(y_test, y_pred, weights='quadratic')","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/team-data/Feb 2021/\"\nimage_size = 128\nframe_rate = 0.1\n\ndef getFrame(sec):\n    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n    hasFrames,image = vidcap.read()\n    if hasFrames:\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.resize(image, (image_size, image_size))\n    return [hasFrames, image]","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = sorted(os.listdir(path))\nnew_list = []\n\nfor xyz in tqdm(files):\n    for mno in os.listdir(path+xyz+'/'):\n        new_list.append(path+xyz+'/'+mno)","execution_count":27,"outputs":[{"output_type":"stream","text":"100%|██████████| 10/10 [00:00<00:00, 288.86it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"for video in new_list:\n    cap = cv2.VideoCapture(video)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    duration = frame_count/fps\n    print(video, '-' ,duration)","execution_count":30,"outputs":[{"output_type":"stream","text":"../input/team-data/Feb 2021/00/thankyou-4.avi - 2.7333333333333334\n../input/team-data/Feb 2021/00/thankyou-17.mp4 - 2.8\n../input/team-data/Feb 2021/00/thankyou-15.mp4 - 2.066666666666667\n../input/team-data/Feb 2021/00/thankyou-10.mp4 - 2.533333333333333\n../input/team-data/Feb 2021/00/thankyou-6.mp4 - 2.4\n../input/team-data/Feb 2021/00/thankyou-9.mp4 - 2.533333333333333\n../input/team-data/Feb 2021/00/thank-you-1.mp4 - 2.3023000000000002\n../input/team-data/Feb 2021/00/thank you-3.mp4 - 3.003\n../input/team-data/Feb 2021/00/thankyou-7.mp4 - 4.866666666666666\n../input/team-data/Feb 2021/00/thankyou-13.mp4 - 2.066666666666667\n../input/team-data/Feb 2021/00/thankyou-19.mp4 - 2.2\n../input/team-data/Feb 2021/00/thankyou-18.mp4 - 2.1333333333333333\n../input/team-data/Feb 2021/00/thankyou-14.mp4 - 2.3333333333333335\n../input/team-data/Feb 2021/00/thank you-2.mp4 - 1.7934583333333332\n../input/team-data/Feb 2021/00/thankyou-20.mp4 - 1.9333333333333333\n../input/team-data/Feb 2021/00/thankyou-22.mp4 - 1.8666666666666667\n../input/team-data/Feb 2021/00/Thank you-23.mp4 - 2.68\n../input/team-data/Feb 2021/00/thankyou-5.mp4 - 3.7333333333333334\n../input/team-data/Feb 2021/00/thankyou-16.mp4 - 3.3333333333333335\n../input/team-data/Feb 2021/00/thankyou-8.mp4 - 3.2666666666666666\n../input/team-data/Feb 2021/00/thankyou-12.mp4 - 2.2666666666666666\n../input/team-data/Feb 2021/00/thankyou-21.mp4 - 2.066666666666667\n../input/team-data/Feb 2021/00/thankyou-11.mp4 - 2.4\n../input/team-data/Feb 2021/01/goodbye-3.mp4 - 5.005\n../input/team-data/Feb 2021/01/good-bye-2.mp4 - 3.003\n../input/team-data/Feb 2021/01/good-bye-1.mp4 - 2.3023000000000002\n../input/team-data/Feb 2021/01/goodbye-4.mp4 - 3.0\n../input/team-data/Feb 2021/02/good-morning-2.mp4 - 2.8\n../input/team-data/Feb 2021/02/good morning-4.mp4 - 4.3043000000000005\n../input/team-data/Feb 2021/02/good morning-5.mp4 - 3.0\n../input/team-data/Feb 2021/02/good-morning-1.mp4 - 2.7\n../input/team-data/Feb 2021/02/good morning-6.mp4 - 2.68\n../input/team-data/Feb 2021/02/good-morning-3.mp4 - 3.1\n../input/team-data/Feb 2021/03/hello-16.mp4 - 1.8\n../input/team-data/Feb 2021/03/hello-22.mp4 - 2.7333333333333334\n../input/team-data/Feb 2021/03/hello-10.mp4 - 2.2\n../input/team-data/Feb 2021/03/hello-20.mp4 - 2.066666666666667\n../input/team-data/Feb 2021/03/hello-12.mp4 - 2.466666666666667\n../input/team-data/Feb 2021/03/hello-3.mp4 - 3.0030030030030033\n../input/team-data/Feb 2021/03/hello-1.mp4 - 1.8351666666666666\n../input/team-data/Feb 2021/03/hello-4.mp4 - 3.003\n../input/team-data/Feb 2021/03/hello-17.mp4 - 1.4666666666666666\n../input/team-data/Feb 2021/03/hello-2.mp4 - 2.7027\n../input/team-data/Feb 2021/03/hello-11.mp4 - 1.8\n../input/team-data/Feb 2021/03/hello-13.mp4 - 1.8666666666666667\n../input/team-data/Feb 2021/03/hello-15.mp4 - 1.2666666666666666\n../input/team-data/Feb 2021/03/hello-6.mp4 - 2.28\n../input/team-data/Feb 2021/03/hello-24.mp4 - 1.6\n../input/team-data/Feb 2021/03/hello-5.mp4 - 3.066666666666667\n../input/team-data/Feb 2021/03/hello-18.mp4 - 1.7333333333333334\n../input/team-data/Feb 2021/03/hello-9.mp4 - 2.3333333333333335\n../input/team-data/Feb 2021/03/hello-21.mp4 - 1.9333333333333333\n../input/team-data/Feb 2021/03/hello 8.mp4 - 1.52\n../input/team-data/Feb 2021/03/hello-14.mp4 - 1.6\n../input/team-data/Feb 2021/03/hello-23.mp4 - 3.066666666666667\n../input/team-data/Feb 2021/03/hello-7.mp4 - 2.3356666666666666\n../input/team-data/Feb 2021/03/hello-19.mp4 - 1.0666666666666667\n../input/team-data/Feb 2021/04/howareyou-10.mp4 - 1.7333333333333334\n../input/team-data/Feb 2021/04/howareyou-7.mp4 - 2.933333333333333\n../input/team-data/Feb 2021/04/how are you-5.mp4 - 3.003\n../input/team-data/Feb 2021/04/howareyou-14.mp4 - 2.1333333333333333\n../input/team-data/Feb 2021/04/howareyou-12.mp4 - 2.0\n../input/team-data/Feb 2021/04/howareyou-8.mp4 - 2.2\n../input/team-data/Feb 2021/04/howareyou-9.mp4 - 2.0\n../input/team-data/Feb 2021/04/how-are-you-2.mp4 - 2.769433333333333\n../input/team-data/Feb 2021/04/howareyou-13.mp4 - 2.466666666666667\n../input/team-data/Feb 2021/04/how-are-you-1.mp4 - 3.003\n../input/team-data/Feb 2021/04/how are you-3.mp4 - 1.9185833333333333\n../input/team-data/Feb 2021/04/howareyou-11.mp4 - 1.7333333333333334\n../input/team-data/Feb 2021/04/how are you-4.mp4 - 2.0020000000000002\n../input/team-data/Feb 2021/04/howareyou-6.mp4 - 2.6666666666666665\n../input/team-data/Feb 2021/05/i am sorry 2.mp4 - 2.8\n../input/team-data/Feb 2021/05/i am sorry 1.mp4 - 3.04\n../input/team-data/Feb 2021/06/i_m fine 1.mp4 - 2.36\n../input/team-data/Feb 2021/07/nicetomeetyou-6.mp4 - 4.0\n../input/team-data/Feb 2021/07/nice-to-meet-you-2.mp4 - 4.0040000000000004\n../input/team-data/Feb 2021/07/nicetomeetyou-11.mp4 - 3.0\n../input/team-data/Feb 2021/07/nicetomeetyou-12.mp4 - 3.6\n../input/team-data/Feb 2021/07/nicetomeetyou-4.mp4 - 3.8\n../input/team-data/Feb 2021/07/nice to meet you-3.mp4 - 4.637966666666666\n../input/team-data/Feb 2021/07/nicetomeetyou-20.mp4 - 2.2666666666666666\n../input/team-data/Feb 2021/07/nicetomeetyou-5.mp4 - 2.6\n../input/team-data/Feb 2021/07/nicetomeetyou-14.mp4 - 2.4\n../input/team-data/Feb 2021/07/nicetomeetyou-16.mp4 - 2.7333333333333334\n../input/team-data/Feb 2021/07/nicetomeetyou-13.mp4 - 3.4\n../input/team-data/Feb 2021/07/nicetomeetyou-9.mp4 - 3.3333333333333335\n../input/team-data/Feb 2021/07/nicetomeetyou-7.mp4 - 2.3333333333333335\n../input/team-data/Feb 2021/07/nicetomeetyou-10.mp4 - 2.2666666666666666\n../input/team-data/Feb 2021/07/nicetomeetyou-18.mp4 - 2.2\n../input/team-data/Feb 2021/07/nice-to-meet-you-1.mp4 - 5.7057\n../input/team-data/Feb 2021/07/nicetomeetyou-8.mp4 - 1.6666666666666667\n../input/team-data/Feb 2021/07/nicetomeetyou-17.mp4 - 2.0\n../input/team-data/Feb 2021/07/nicetomeetyou-19.mp4 - 4.466666666666667\n../input/team-data/Feb 2021/07/nicetomeetyou-15.mp4 - 1.8\n../input/team-data/Feb 2021/08/please-13.mp4 - 1.8666666666666667\n../input/team-data/Feb 2021/08/please-15.mp4 - 2.6666666666666665\n../input/team-data/Feb 2021/08/please-16.mp4 - 3.533333333333333\n../input/team-data/Feb 2021/08/please-18.mp4 - 3.4\n../input/team-data/Feb 2021/08/please-4.mp4 - 2.8\n../input/team-data/Feb 2021/08/please-10.mp4 - 1.7333333333333334\n../input/team-data/Feb 2021/08/please-8.mp4 - 3.1333333333333333\n../input/team-data/Feb 2021/08/please-12.mp4 - 3.533333333333333\n../input/team-data/Feb 2021/08/please-3.mp4 - 2.68\n../input/team-data/Feb 2021/08/please-5.mp4 - 2.066666666666667\n../input/team-data/Feb 2021/08/please-7.mp4 - 2.6\n../input/team-data/Feb 2021/08/please-1.mp4 - 3.0029999999999997\n../input/team-data/Feb 2021/08/please-6.mp4 - 1.9333333333333333\n../input/team-data/Feb 2021/08/please-17.mp4 - 2.2\n../input/team-data/Feb 2021/08/please-2.mp4 - 3.003\n../input/team-data/Feb 2021/08/please-9.mp4 - 1.8\n../input/team-data/Feb 2021/08/please-14.mp4 - 2.1333333333333333\n../input/team-data/Feb 2021/08/please-11.mp4 - 1.5333333333333334\n../input/team-data/Feb 2021/09/take-care-4.mp4 - 2.6026000000000002\n../input/team-data/Feb 2021/09/take-care-1.mp4 - 4.045708333333333\n../input/team-data/Feb 2021/09/take-care-2.mp4 - 1.8351666666666666\n../input/team-data/Feb 2021/09/take-care-3.mp4 - 3.1031\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = []\nlabels = []\n\nfor video in tqdm(new_list):\n    vidcap = cv2.VideoCapture(video)\n    sec = 0\n    mini = []\n    success, frame = getFrame(sec)\n    while success:\n        mini.append(frame)\n        sec = sec + frame_rate\n        sec = round(sec, 2)\n        success, frame = getFrame(sec)\n        \n    mini = mini[-12:]\n    frames.extend(mini)\n    labels.extend([int(video[28:30])]*len(mini))","execution_count":null,"outputs":[{"output_type":"stream","text":" 16%|█▌        | 18/116 [00:09<00:42,  2.30it/s]","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = numpy.array(frames)\nlabels = numpy.array(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(frames.shape)\nprint(labels.shape)","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"(1391, 128, 128, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(frames, labels, test_size=0.1,\n                                                   shuffle=True, random_state=42, stratify=labels)","execution_count":39,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [116, 1392]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-dd1f6860a6b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m x_train, x_test, y_train, y_test = train_test_split(frames, labels, test_size=0.1,\n\u001b[0;32m----> 6\u001b[0;31m                                                    shuffle=True, random_state=42, stratify=labels)\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[1;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 263\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [116, 1392]"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2]*x_train.shape[3])\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2]*x_test.shape[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)\nacc = accuracy_score(y_test, y_pred)\nacc","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}